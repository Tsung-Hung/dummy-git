{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tsung-Hung/dummy-git/blob/master/20230316.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/68478856/pytorch-batchnorm2d-calculation\n",
        "# https://discuss.pytorch.org/t/how-to-use-scripting-with-custom-batchnorm/85375/6\n",
        "# https://blog.csdn.net/qq_39208832/article/details/117930625\n",
        "# https://yichengsu.github.io/2019/12/pytorch-batchnorm-freeze/\n",
        "# pytorch BatchNorm参数详解，计算过程 - 水木清扬 - 博客园\n",
        "# https://www.zhihu.com/question/487766088"
      ],
      "metadata": {
        "id": "GIwuLPOm7IhJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.batchnorm import BatchNorm1d\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "5H99Fxjy7Yci"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "batch, sentence_length, embedding_dim = 2, 3, 4\n",
        "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
        "print(embedding)\n",
        "layer_norm = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "# Activate module\n",
        "origin = layer_norm(embedding)\n",
        "print(origin)\n",
        "print(layer_norm.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYsKHJW27PeB",
        "outputId": "5268e30f-76be-4b88-d0e7-76ecd6bfa260"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.1258, -1.1524, -0.2506, -0.4339],\n",
            "         [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
            "         [ 0.4681, -0.1577,  1.4437,  0.2660]],\n",
            "\n",
            "        [[ 0.1665,  0.8744, -0.1435, -0.1116],\n",
            "         [ 0.9318,  1.2590,  2.0050,  0.0537],\n",
            "         [ 0.6181, -0.4128, -0.8411, -2.3160]]])\n",
            "tensor([[[-0.9539, -1.0196,  1.2137,  0.7598],\n",
            "         [ 0.9075,  0.7747, -0.0791, -1.6031],\n",
            "         [-0.0629, -1.1288,  1.5988, -0.4070]],\n",
            "\n",
            "        [[-0.0732,  1.6553, -0.8299, -0.7521],\n",
            "         [-0.1864,  0.2808,  1.3460, -1.4403],\n",
            "         [ 1.2863,  0.3084, -0.0978, -1.4969]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "OrderedDict([('weight', tensor([1., 1., 1., 1.])), ('bias', tensor([0., 0., 0., 0.]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Layer_norm(x):\n",
        "    '''\n",
        "    BatchNorm2d\n",
        "    test = Layer_norm1(embedding)\n",
        "    test\n",
        "    '''\n",
        "    batch, sentence_length, embedding_dim = x.shape\n",
        "    x = x.reshape(1, batch*sentence_length, embedding_dim, 1)\n",
        "    layer_norm = nn.BatchNorm2d(x.shape[1], track_running_stats=True, affine=True)\n",
        "    output = layer_norm(x)\n",
        "    output = output.reshape(output.shape[1], output.shape[2]).reshape(batch, sentence_length, embedding_dim)\n",
        "    return output\n",
        "\n",
        "def Layer_norm2(x):\n",
        "    '''\n",
        "    BatchNorm1d\n",
        "    test = Layer_norm2(embedding)\n",
        "    test\n",
        "    '''\n",
        "    batch, sentence_length, embedding_dim = x.shape\n",
        "    x = x.reshape(1, batch*sentence_length, embedding_dim)\n",
        "    layer_norm = nn.BatchNorm1d(x.shape[1], track_running_stats=True, affine=True, momentum=0)\n",
        "\n",
        "    # Set the running statistics to constant\n",
        "    layer_norm.running_mean.fill_(0)\n",
        "    layer_norm.running_var.fill_(1)\n",
        "    output = layer_norm(x)\n",
        "    output = output.reshape(output.shape[1], output.shape[2]).reshape(batch, sentence_length, embedding_dim)\n",
        "    return output\n",
        "\n",
        "class LayerNorm2(nn.Module):\n",
        "    '''\n",
        "    layer_norm2 = LayerNorm2(embedding_dim)\n",
        "    # Activate module\n",
        "    test = layer_norm2(embedding)\n",
        "    test\n",
        "    '''\n",
        "    def __init__(self, num_features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(num_features), requires_grad=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(num_features), requires_grad=False)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape = x.shape\n",
        "        mean = x.mean(-1)\n",
        "        std = torch.sqrt(x.var(-1, unbiased=False))\n",
        "        res = torch.stack([((x[i][j] - mean[i][j]).squeeze(0) / (std[i][j].squeeze(0) + self.eps)) for i in range(x.shape[0]) for j in range(x.shape[1])], dim=0)\n",
        "        return res.reshape(shape)\n",
        "\n",
        "class LayerNorm_sim(nn.Module):\n",
        "    '''\n",
        "    Provided by \"Gina\" \n",
        "    layer_norm_sim = LayerNorm_sim(embedding_dim)\n",
        "    # Activate module\n",
        "    test = layer_norm_sim(embedding)\n",
        "    test\n",
        "    '''\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.BN = nn.BatchNorm1d(num_features, momentum=1, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm_list = []\n",
        "        for i in range(x.shape[0]):\n",
        "          x_batch = x[i, ...].unsqueeze(0)\n",
        "          x_norm_list.append(self.BN(x_batch))\n",
        "        return torch.cat(x_norm_list, dim=0)\n",
        "\n",
        "layer_norm_sim = LayerNorm_sim(3)\n",
        "# Activate module\n",
        "test = layer_norm_sim(embedding)\n",
        "layer_norm_sim.BN.state_dict()\n",
        "\n",
        "test = Layer_norm2(embedding)\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EpXSuC-7S57",
        "outputId": "970fdbbe-b1f5-4b87-d15b-23c50619068b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.9539, -1.0196,  1.2137,  0.7598],\n",
              "         [ 0.9075,  0.7747, -0.0791, -1.6031],\n",
              "         [-0.0629, -1.1288,  1.5988, -0.4070]],\n",
              "\n",
              "        [[-0.0732,  1.6553, -0.8299, -0.7521],\n",
              "         [-0.1864,  0.2808,  1.3460, -1.4403],\n",
              "         [ 1.2863,  0.3084, -0.0978, -1.4969]]],\n",
              "       grad_fn=<ReshapeAliasBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layernorm"
      ],
      "metadata": {
        "id": "Fa10UWSqq4Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "# Define the SimpleNet model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(64, elementwise_affine = True)\n",
        "        self.fc1 = nn.Linear(640, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x) #input(1,10,64)\n",
        "        x = x.view(-1, 640)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Generate a simulated dataset\n",
        "inputs = torch.rand(3, 10, 64)\n",
        "raw_inputs = inputs\n",
        "labels = torch.randint(low=0, high=10, size=(3,))\n",
        "inputs = torch.Tensor(inputs)\n",
        "labels = torch.Tensor(labels).long()\n",
        "train_dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
        "\n",
        "# Create a DataLoader to load and batch the data\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=False)\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "net = SimpleNet()\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(25):\n",
        "    running_loss = 0.0\n",
        "    #print(\"1. Running mean:\", net.bn1.running_mean)\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        #print(\"2. Running mean:\", net.bn1.running_mean)\n",
        "        #print(\"2. Running var:\", net.bn1.running_var)\n",
        "    print('Epoch [{}/50], Loss: {:.4f}'.format(epoch+1, running_loss / len(train_loader)))\n",
        "    #print(\"3. Running mean:\", net.bn1.running_mean)\n",
        "    print(\"===========================\")\n",
        "\n",
        "print('\\n', net.ln1.state_dict(), '\\n')\n",
        "\n",
        "torch.manual_seed(1)\n",
        "test_sample = torch.randn(1, 10, 64)\n",
        "net.eval()\n",
        "output = net(test_sample)\n",
        "print('test results: ', output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEJGo1PA7e-y",
        "outputId": "5062944e-65c3-49b4-bfad-37509203782e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 7.3685\n",
            "===========================\n",
            "Epoch [2/50], Loss: 7.1339\n",
            "===========================\n",
            "Epoch [3/50], Loss: 6.6881\n",
            "===========================\n",
            "Epoch [4/50], Loss: 6.0521\n",
            "===========================\n",
            "Epoch [5/50], Loss: 5.2450\n",
            "===========================\n",
            "Epoch [6/50], Loss: 4.2836\n",
            "===========================\n",
            "Epoch [7/50], Loss: 3.1832\n",
            "===========================\n",
            "Epoch [8/50], Loss: 1.9571\n",
            "===========================\n",
            "Epoch [9/50], Loss: 1.3168\n",
            "===========================\n",
            "Epoch [10/50], Loss: 1.1649\n",
            "===========================\n",
            "Epoch [11/50], Loss: 1.8080\n",
            "===========================\n",
            "Epoch [12/50], Loss: 2.6305\n",
            "===========================\n",
            "Epoch [13/50], Loss: 3.1412\n",
            "===========================\n",
            "Epoch [14/50], Loss: 3.3626\n",
            "===========================\n",
            "Epoch [15/50], Loss: 3.3238\n",
            "===========================\n",
            "Epoch [16/50], Loss: 3.0515\n",
            "===========================\n",
            "Epoch [17/50], Loss: 2.5698\n",
            "===========================\n",
            "Epoch [18/50], Loss: 1.9009\n",
            "===========================\n",
            "Epoch [19/50], Loss: 1.2984\n",
            "===========================\n",
            "Epoch [20/50], Loss: 1.1313\n",
            "===========================\n",
            "Epoch [21/50], Loss: 1.3037\n",
            "===========================\n",
            "Epoch [22/50], Loss: 1.5062\n",
            "===========================\n",
            "Epoch [23/50], Loss: 1.7159\n",
            "===========================\n",
            "Epoch [24/50], Loss: 1.8933\n",
            "===========================\n",
            "Epoch [25/50], Loss: 1.8772\n",
            "===========================\n",
            "\n",
            " OrderedDict([('weight', tensor([0.9952, 0.9935, 0.9953, 0.9907, 0.9962, 0.9912, 0.9938, 0.9912, 0.9970,\n",
            "        0.9944, 0.9961, 0.9965, 0.9958, 0.9965, 0.9892, 0.9986, 0.9935, 0.9935,\n",
            "        0.9964, 0.9940, 0.9986, 0.9895, 0.9967, 0.9966, 0.9941, 0.9950, 0.9945,\n",
            "        0.9918, 0.9935, 0.9931, 0.9944, 0.9934, 0.9971, 0.9978, 0.9971, 0.9941,\n",
            "        0.9934, 0.9966, 0.9961, 0.9956, 0.9956, 0.9847, 0.9930, 0.9921, 0.9938,\n",
            "        0.9947, 0.9958, 0.9952, 0.9958, 0.9933, 0.9918, 0.9958, 0.9909, 0.9941,\n",
            "        0.9972, 0.9931, 0.9916, 0.9937, 0.9946, 0.9968, 0.9963, 0.9965, 0.9957,\n",
            "        0.9970])), ('bias', tensor([ 3.7874e-03,  3.1419e-03,  7.0227e-04, -2.8450e-03,  3.4936e-04,\n",
            "        -9.7032e-03, -3.2013e-03, -7.4943e-03,  5.0459e-04,  2.2038e-03,\n",
            "         4.1029e-03, -1.8574e-03, -5.9623e-05,  1.8579e-03,  1.8033e-03,\n",
            "        -2.3814e-03,  1.2775e-03, -1.0071e-02, -2.8179e-03,  1.9299e-03,\n",
            "         2.9579e-03,  6.4886e-03, -6.3134e-04,  5.5695e-04, -6.1175e-04,\n",
            "         4.9529e-04,  3.3187e-05,  6.1492e-03, -1.0980e-03,  2.0306e-03,\n",
            "         4.6915e-03, -1.0647e-03,  1.3202e-03,  7.7493e-04,  3.0901e-03,\n",
            "         2.9408e-03,  8.2598e-04, -1.9243e-03, -1.2456e-03, -2.0893e-03,\n",
            "        -5.1935e-03,  1.1273e-02, -6.6850e-03,  4.0856e-03, -2.3403e-03,\n",
            "        -7.1223e-03, -1.0875e-03, -2.8244e-03,  3.4255e-03,  1.7975e-03,\n",
            "        -6.6799e-03,  2.5611e-04, -1.0118e-03, -5.3771e-04, -2.6478e-04,\n",
            "         4.3074e-03, -7.5705e-03,  8.5368e-03, -5.6004e-04, -3.4558e-03,\n",
            "         7.3990e-04,  1.6402e-03, -2.2815e-03, -5.8503e-03]))]) \n",
            "\n",
            "test results:  tensor([[-1.1568]], grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replace layernorm with batchnorm"
      ],
      "metadata": {
        "id": "SKBB80C4q-ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "# Define the SimpleNet model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.bn1 = BatchNorm1d(10, track_running_stats=True, affine=False, momentum=1)\n",
        "        self.fc1 = nn.Linear(640, 1)\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.ones(64))\n",
        "        self.beta = nn.Parameter(torch.zeros(64))\n",
        "\n",
        "    def Layer_norm1(self, x):\n",
        "        batch, sentence_length, embedding_dim = x.shape\n",
        "        x = x.reshape(1, batch*sentence_length, embedding_dim)\n",
        "\n",
        "        output = self.bn1(x) \n",
        "        output = output.reshape(output.shape[1], output.shape[2]).reshape(batch, sentence_length, embedding_dim)\n",
        "\n",
        "        #apply gamma and beta\n",
        "        return self.gamma * output + self.beta\n",
        "\n",
        "\n",
        "    def loop_layernorm1(self, x):      \n",
        "        split_x = torch.split(x,1,dim=0)\n",
        "        concate_list = []\n",
        "        for small_x in split_x:\n",
        "            #small_x = small_x.squeeze().permute(1,0)\n",
        "            #small_x = self.bn1(small_x).permute(1,0)\n",
        "            small_x = self.bn1(small_x)\n",
        "            concate_list.append(small_x)\n",
        "    \n",
        "        concate = torch.stack(concate_list)\n",
        "        return self.gamma * concate + self.beta\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.loop_layernorm1(x) #input(1,10,64)\n",
        "        x = x.reshape(-1, 640)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Generate a simulated dataset\n",
        "inputs = torch.rand(3, 10, 64)\n",
        "raw_inputs = inputs\n",
        "labels = torch.randint(low=0, high=10, size=(3,))\n",
        "inputs = torch.Tensor(inputs)\n",
        "labels = torch.Tensor(labels).long()\n",
        "train_dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
        "\n",
        "# Create a DataLoader to load and batch the data\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=False)\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "net = SimpleNet()\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(25):\n",
        "    running_loss = 0.0\n",
        "    #print(\"1. Running mean:\", net.bn2.state_dict())\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        #print(\"2. Running mean:\", net.bn1.running_mean)\n",
        "        #print(\"2. Running var:\", net.bn1.running_var)\n",
        "    print('Epoch [{}/50], Loss: {:.4f}'.format(epoch+1, running_loss / len(train_loader)))\n",
        "    #print(\"3. Running mean:\", net.bn1.running_mean)\n",
        "    print(\"===========================\")\n",
        "\n",
        "net.bn1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O0k27Uo7TRE",
        "outputId": "6343393a-a0e1-4fc9-e707-9a0ed2ef500b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 7.3685\n",
            "===========================\n",
            "Epoch [2/50], Loss: 7.1339\n",
            "===========================\n",
            "Epoch [3/50], Loss: 6.6881\n",
            "===========================\n",
            "Epoch [4/50], Loss: 6.0521\n",
            "===========================\n",
            "Epoch [5/50], Loss: 5.2450\n",
            "===========================\n",
            "Epoch [6/50], Loss: 4.2836\n",
            "===========================\n",
            "Epoch [7/50], Loss: 3.1832\n",
            "===========================\n",
            "Epoch [8/50], Loss: 1.9571\n",
            "===========================\n",
            "Epoch [9/50], Loss: 1.3168\n",
            "===========================\n",
            "Epoch [10/50], Loss: 1.1649\n",
            "===========================\n",
            "Epoch [11/50], Loss: 1.8080\n",
            "===========================\n",
            "Epoch [12/50], Loss: 2.6305\n",
            "===========================\n",
            "Epoch [13/50], Loss: 3.1412\n",
            "===========================\n",
            "Epoch [14/50], Loss: 3.3626\n",
            "===========================\n",
            "Epoch [15/50], Loss: 3.3238\n",
            "===========================\n",
            "Epoch [16/50], Loss: 3.0515\n",
            "===========================\n",
            "Epoch [17/50], Loss: 2.5698\n",
            "===========================\n",
            "Epoch [18/50], Loss: 1.9009\n",
            "===========================\n",
            "Epoch [19/50], Loss: 1.2984\n",
            "===========================\n",
            "Epoch [20/50], Loss: 1.1313\n",
            "===========================\n",
            "Epoch [21/50], Loss: 1.3037\n",
            "===========================\n",
            "Epoch [22/50], Loss: 1.5062\n",
            "===========================\n",
            "Epoch [23/50], Loss: 1.7159\n",
            "===========================\n",
            "Epoch [24/50], Loss: 1.8933\n",
            "===========================\n",
            "Epoch [25/50], Loss: 1.8772\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('running_mean',\n",
              "              tensor([0.4767, 0.5704, 0.4466, 0.5212, 0.4753, 0.5117, 0.5128, 0.5090, 0.5241,\n",
              "                      0.5205])),\n",
              "             ('running_var',\n",
              "              tensor([0.0769, 0.0829, 0.0836, 0.0928, 0.0749, 0.0774, 0.0873, 0.0764, 0.0794,\n",
              "                      0.0868])),\n",
              "             ('num_batches_tracked', tensor(75))])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_bn_training(m):\n",
        "    if isinstance(m, nn.BatchNorm1d):\n",
        "        m.train()\n",
        "\n",
        "torch.manual_seed(1)\n",
        "test_sample = torch.randn(1, 10, 64)\n",
        "#print(test_sample)\n",
        "#print(torch.mean(test_sample, dim=2))\n",
        "net.eval()\n",
        "print(net.bn1.training)\n",
        "print(\"After Running\", net.bn1.state_dict())\n",
        "\n",
        "net.apply(set_bn_training)\n",
        "output = net(test_sample)\n",
        "print(\"Before Running\", net.bn1.state_dict())\n",
        "print(net.bn1.training)\n",
        "\n",
        "print('\\n', '-----------------------')\n",
        "print(net.bn1.state_dict())\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rkk_s6J7Z3S",
        "outputId": "689365fd-a475-4932-dfbb-cc528f322c5b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "After Running OrderedDict([('running_mean', tensor([0.4767, 0.5704, 0.4466, 0.5212, 0.4753, 0.5117, 0.5128, 0.5090, 0.5241,\n",
            "        0.5205])), ('running_var', tensor([0.0769, 0.0829, 0.0836, 0.0928, 0.0749, 0.0774, 0.0873, 0.0764, 0.0794,\n",
            "        0.0868])), ('num_batches_tracked', tensor(75))])\n",
            "Before Running OrderedDict([('running_mean', tensor([-0.1130, -0.0192,  0.1874,  0.1517, -0.0068, -0.0377, -0.0822, -0.1019,\n",
            "         0.1284, -0.0743])), ('running_var', tensor([1.0302, 0.9497, 1.2041, 1.1384, 1.1395, 0.9414, 0.9778, 1.0286, 0.6879,\n",
            "        1.3984])), ('num_batches_tracked', tensor(76))])\n",
            "True\n",
            "\n",
            " -----------------------\n",
            "OrderedDict([('running_mean', tensor([-0.1130, -0.0192,  0.1874,  0.1517, -0.0068, -0.0377, -0.0822, -0.1019,\n",
            "         0.1284, -0.0743])), ('running_var', tensor([1.0302, 0.9497, 1.2041, 1.1384, 1.1395, 0.9414, 0.9778, 1.0286, 0.6879,\n",
            "        1.3984])), ('num_batches_tracked', tensor(76))])\n",
            "tensor([[-1.1568]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Batchnorm\n",
        "torch.manual_seed(1)\n",
        "test_sample = torch.randn(1, 10, 64)\n",
        "\n",
        "mean = torch.mean(test_sample, dim=2).reshape(1,10,1)\n",
        "var = torch.var(test_sample, dim=2, unbiased=False).reshape(1,10,1)\n",
        "\n",
        "norm = (test_sample-mean)/torch.sqrt(var + net.bn1.eps)\n",
        "#norm = (test_sample-net.bn1.running_mean.reshape(1,10,1))/torch.sqrt(net.bn1.running_var.reshape(1,10,1)*63/64 + net.bn1.eps)\n",
        "\n",
        "#test = net.bn1(test_sample)\n",
        "results = net.gamma * norm + net.beta\n",
        "net.fc1(results.reshape(-1,640))\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-mSBcFi7chi",
        "outputId": "89fa9e5f-2733-4a46-b9cc-f0e8444bf638"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.1568]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replace layernorm with batchnorm 2"
      ],
      "metadata": {
        "id": "m7ZoVoMGruAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "# Defince the customer batchnorm\n",
        "class CustomBatchNorm(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=1):\n",
        "        super(CustomBatchNorm, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Initialize learnable parameters\n",
        "        #self.gamma = nn.Parameter(torch.ones(num_features), requires_grad=False)\n",
        "        #self.beta = nn.Parameter(torch.zeros(num_features), requires_grad=False)\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "        self.register_buffer('running_var', torch.ones(num_features))\n",
        "\n",
        "\n",
        "    def forward_(self, x):\n",
        "        # Calculate batch statistics\n",
        "        batch_mean = x.mean(dim=2)\n",
        "        batch_var = x.var(dim=2, unbiased=False)\n",
        "\n",
        "        # Update running statistics\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "        # Normalize input\n",
        "        x = (x - batch_mean.unsqueeze(dim=2)) / torch.sqrt(batch_var.unsqueeze(dim=2) + self.eps)\n",
        "\n",
        "        # Apply scale and shift\n",
        "        #x = self.gamma * x + self.beta\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate batch statistics\n",
        "        # if self.training:\n",
        "\n",
        "        #     batch_mean = x.mean([0, 2])\n",
        "        #     batch_var = x.var([0, 2], unbiased=False)\n",
        "\n",
        "        #     # Update running statistics\n",
        "        #     with torch.no_grad():\n",
        "        #         self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "        #         self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "        # else:\n",
        "        #     batch_mean = x.mean([0, 2])\n",
        "        #     batch_var = x.var([0, 2], unbiased=False)\n",
        "        #     with torch.no_grad():\n",
        "        #         self.running_mean = batch_mean\n",
        "        #         self.running_var = batch_var\n",
        "           \n",
        "        # Normalize input\n",
        "        x = F.batch_norm(x, running_mean=self.running_mean, running_var=self.running_var, \n",
        "                  training=True, momentum=self.momentum, eps=self.eps)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define the SimpleNet model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        #self.bn1 = BatchNorm1d(10, track_running_stats=True, affine=False, momentum=1)\n",
        "        self.bn1 = CustomBatchNorm(10)\n",
        "        self.fc1 = nn.Linear(640, 1)\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.ones(64))\n",
        "        self.beta = nn.Parameter(torch.zeros(64))\n",
        "\n",
        "    def Layer_norm1(self, x):\n",
        "        batch, sentence_length, embedding_dim = x.shape\n",
        "        x = x.reshape(1, batch*sentence_length, embedding_dim)\n",
        "\n",
        "        output = self.bn1(x) \n",
        "        output = output.reshape(output.shape[1], output.shape[2]).reshape(batch, sentence_length, embedding_dim)\n",
        "\n",
        "        #apply gamma and beta\n",
        "        return self.gamma * output + self.beta\n",
        "\n",
        "\n",
        "    def loop_layernorm1(self, x):      \n",
        "        split_x = torch.split(x,1,dim=0)\n",
        "        concate_list = []\n",
        "        for small_x in split_x:\n",
        "            small_x = self.bn1(small_x)\n",
        "            concate_list.append(small_x)\n",
        "    \n",
        "        concate = torch.cat(concate_list)\n",
        "        return self.gamma * concate + self.beta\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.loop_layernorm1(x) #input(1,10,64)\n",
        "        x = x.reshape(-1, 640)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Generate a simulated dataset\n",
        "inputs = torch.rand(3, 10, 64)\n",
        "raw_inputs = inputs\n",
        "labels = torch.randint(low=0, high=10, size=(3,))\n",
        "inputs = torch.Tensor(inputs)\n",
        "labels = torch.Tensor(labels).long()\n",
        "train_dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
        "\n",
        "# Create a DataLoader to load and batch the data\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=False)\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "net = SimpleNet()\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(2):\n",
        "    running_loss = 0.0\n",
        "    #print(\"1. Running mean:\", net.bn2.state_dict())\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print('Epoch [{}/50], Loss: {:.4f}'.format(epoch+1, running_loss / len(train_loader)))\n",
        "    print(\"===========================\")\n",
        "\n",
        "net.bn1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya-EeDP0_O2K",
        "outputId": "a1a0e5b3-8956-4e64-942c-cbb45da98d16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 7.3685\n",
            "===========================\n",
            "Epoch [2/50], Loss: 7.1339\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('running_mean',\n",
              "              tensor([0.4767, 0.5704, 0.4466, 0.5212, 0.4753, 0.5117, 0.5128, 0.5090, 0.5241,\n",
              "                      0.5205])),\n",
              "             ('running_var',\n",
              "              tensor([0.0769, 0.0829, 0.0836, 0.0928, 0.0749, 0.0774, 0.0873, 0.0764, 0.0794,\n",
              "                      0.0868]))])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "net.eval()\n",
        "\n",
        "torch.manual_seed(1)\n",
        "test_sample = torch.randn(1, 10, 64)\n",
        "\n",
        "\n",
        "output = net(test_sample)\n",
        "print(output)\n",
        "print(net.bn1.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BhL0OisTK0H",
        "outputId": "80729ed6-ab5e-4263-cf82-d6e674bb5b4f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.1568]], grad_fn=<AddmmBackward0>)\n",
            "OrderedDict([('running_mean', tensor([-0.1130, -0.0192,  0.1874,  0.1517, -0.0068, -0.0377, -0.0822, -0.1019,\n",
            "         0.1284, -0.0743])), ('running_var', tensor([1.0302, 0.9497, 1.2041, 1.1384, 1.1395, 0.9414, 0.9778, 1.0286, 0.6879,\n",
            "        1.3984]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export onnx"
      ],
      "metadata": {
        "id": "v1Ow5QPCs4Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "net.eval()\n",
        "x = torch.randn(1, 10, 64) \n",
        "\n",
        "# Export the model to ONNX format\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(net, x,'testtest.onnx',\\\n",
        "    verbose=False,opset_version=15,\\\n",
        "    input_names=[\"input\"], output_names=['output'])"
      ],
      "metadata": {
        "id": "0difIXiWs2E1",
        "outputId": "5fa1c96f-102c-46d3-83ce-27c5ed419644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/onnx/symbolic_helper.py:1457: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'batch_norm' is set to train=True. Exporting with train=True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "歡迎使用 Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}