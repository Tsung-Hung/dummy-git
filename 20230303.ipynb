{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tsung-Hung/dummy-git/blob/master/20230303.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/68478856/pytorch-batchnorm2d-calculation\n",
        "# https://discuss.pytorch.org/t/how-to-use-scripting-with-custom-batchnorm/85375/6\n",
        "# https://blog.csdn.net/qq_39208832/article/details/117930625\n",
        "# https://yichengsu.github.io/2019/12/pytorch-batchnorm-freeze/\n",
        "# pytorch BatchNorm参数详解，计算过程 - 水木清扬 - 博客园\n",
        "# https://www.zhihu.com/question/487766088"
      ],
      "metadata": {
        "id": "GIwuLPOm7IhJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.batchnorm import BatchNorm1d\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "5H99Fxjy7Yci"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "batch, sentence_length, embedding_dim = 2, 3, 4\n",
        "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
        "print(embedding)\n",
        "layer_norm = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "# Activate module\n",
        "origin = layer_norm(embedding)\n",
        "print(origin)\n",
        "print(layer_norm.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYsKHJW27PeB",
        "outputId": "ab30c851-b052-44e4-d761-8df05f224764"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.1258, -1.1524, -0.2506, -0.4339],\n",
            "         [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
            "         [ 0.4681, -0.1577,  1.4437,  0.2660]],\n",
            "\n",
            "        [[ 0.1665,  0.8744, -0.1435, -0.1116],\n",
            "         [ 0.9318,  1.2590,  2.0050,  0.0537],\n",
            "         [ 0.6181, -0.4128, -0.8411, -2.3160]]])\n",
            "tensor([[[-0.9539, -1.0196,  1.2137,  0.7598],\n",
            "         [ 0.9075,  0.7747, -0.0791, -1.6031],\n",
            "         [-0.0629, -1.1288,  1.5988, -0.4070]],\n",
            "\n",
            "        [[-0.0732,  1.6553, -0.8299, -0.7521],\n",
            "         [-0.1864,  0.2808,  1.3460, -1.4403],\n",
            "         [ 1.2863,  0.3084, -0.0978, -1.4969]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "OrderedDict([('weight', tensor([1., 1., 1., 1.])), ('bias', tensor([0., 0., 0., 0.]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Layer_norm(x):\n",
        "    '''\n",
        "    BatchNorm2d\n",
        "    test = Layer_norm1(embedding)\n",
        "    test\n",
        "    '''\n",
        "    batch, sentence_length, embedding_dim = x.shape\n",
        "    x = x.reshape(1, batch*sentence_length, embedding_dim, 1)\n",
        "    layer_norm = nn.BatchNorm2d(x.shape[1], track_running_stats=True, affine=True)\n",
        "    output = layer_norm(x)\n",
        "    output = output.reshape(output.shape[1], output.shape[2]).reshape(batch, sentence_length, embedding_dim)\n",
        "    return output\n",
        "\n",
        "def Layer_norm2(x):\n",
        "    '''\n",
        "    BatchNorm1d\n",
        "    test = Layer_norm2(embedding)\n",
        "    test\n",
        "    '''\n",
        "    batch, sentence_length, embedding_dim = x.shape\n",
        "    x = x.reshape(1, batch*sentence_length, embedding_dim)\n",
        "    layer_norm = nn.BatchNorm1d(x.shape[1], track_running_stats=True, affine=True, momentum=0)\n",
        "\n",
        "    # Set the running statistics to constant\n",
        "    layer_norm.running_mean.fill_(0)\n",
        "    layer_norm.running_var.fill_(1)\n",
        "    output = layer_norm(x)\n",
        "    output = output.reshape(output.shape[1], output.shape[2]).reshape(batch, sentence_length, embedding_dim)\n",
        "    return output\n",
        "\n",
        "class LayerNorm2(nn.Module):\n",
        "    '''\n",
        "    layer_norm2 = LayerNorm2(embedding_dim)\n",
        "    # Activate module\n",
        "    test = layer_norm2(embedding)\n",
        "    test\n",
        "    '''\n",
        "    def __init__(self, num_features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(num_features), requires_grad=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(num_features), requires_grad=False)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape = x.shape\n",
        "        mean = x.mean(-1)\n",
        "        std = torch.sqrt(x.var(-1, unbiased=False))\n",
        "        res = torch.stack([((x[i][j] - mean[i][j]).squeeze(0) / (std[i][j].squeeze(0) + self.eps)) for i in range(x.shape[0]) for j in range(x.shape[1])], dim=0)\n",
        "        return res.reshape(shape)\n",
        "\n",
        "class LayerNorm_sim(nn.Module):\n",
        "    '''\n",
        "    Provided by \"Gina\" \n",
        "    layer_norm_sim = LayerNorm_sim(embedding_dim)\n",
        "    # Activate module\n",
        "    test = layer_norm_sim(embedding)\n",
        "    test\n",
        "    '''\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.BN = nn.BatchNorm1d(num_features, momentum=1, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm_list = []\n",
        "        for i in range(x.shape[0]):\n",
        "          x_batch = x[i, ...].unsqueeze(0)\n",
        "          x_norm_list.append(self.BN(x_batch))\n",
        "        return torch.cat(x_norm_list, dim=0)\n",
        "\n",
        "layer_norm_sim = LayerNorm_sim(3)\n",
        "# Activate module\n",
        "test = layer_norm_sim(embedding)\n",
        "layer_norm_sim.BN.state_dict()\n",
        "\n",
        "test = Layer_norm2(embedding)\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EpXSuC-7S57",
        "outputId": "60717a39-6e98-4d6f-9319-97a5ec063453"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.9539, -1.0196,  1.2137,  0.7598],\n",
              "         [ 0.9075,  0.7747, -0.0791, -1.6031],\n",
              "         [-0.0629, -1.1288,  1.5988, -0.4070]],\n",
              "\n",
              "        [[-0.0732,  1.6553, -0.8299, -0.7521],\n",
              "         [-0.1864,  0.2808,  1.3460, -1.4403],\n",
              "         [ 1.2863,  0.3084, -0.0978, -1.4969]]],\n",
              "       grad_fn=<ReshapeAliasBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "# Define the SimpleNet model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.bn1 = BatchNorm1d(10, track_running_stats=True, affine=False, momentum=1)\n",
        "        self.fc1 = nn.Linear(640, 1)\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.ones(64))\n",
        "        self.beta = nn.Parameter(torch.zeros(64))\n",
        "\n",
        "    def Layer_norm1(self, x):\n",
        "        batch, sentence_length, embedding_dim = x.shape\n",
        "        x = x.reshape(1, batch*sentence_length, embedding_dim)\n",
        "\n",
        "        output = self.bn1(x) \n",
        "        output = output.reshape(output.shape[1], output.shape[2]).reshape(batch, sentence_length, embedding_dim)\n",
        "\n",
        "        #apply gamma and beta\n",
        "        return self.gamma * output + self.beta\n",
        "\n",
        "\n",
        "    def loop_layernorm1(self, x):      \n",
        "        split_x = torch.split(x,1,dim=0)\n",
        "        concate_list = []\n",
        "        for small_x in split_x:\n",
        "            #small_x = small_x.squeeze().permute(1,0)\n",
        "            #small_x = self.bn1(small_x).permute(1,0)\n",
        "            small_x = self.bn1(small_x)\n",
        "            print(\"_______\")\n",
        "            print(small_x)\n",
        "            concate_list.append(small_x)\n",
        "    \n",
        "        concate = torch.stack(concate_list)\n",
        "        return self.gamma * concate + self.beta\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.loop_layernorm1(x) #input(1,10,64)\n",
        "        x = x.reshape(-1, 640)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Generate a simulated dataset\n",
        "inputs = torch.rand(3, 10, 64)\n",
        "raw_inputs = inputs\n",
        "labels = torch.randint(low=0, high=10, size=(3,))\n",
        "inputs = torch.Tensor(inputs)\n",
        "labels = torch.Tensor(labels).long()\n",
        "train_dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
        "\n",
        "# Create a DataLoader to load and batch the data\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "net = SimpleNet()\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1):\n",
        "    running_loss = 0.0\n",
        "    #print(\"1. Running mean:\", net.bn2.state_dict())\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        #print(\"2. Running mean:\", net.bn1.running_mean)\n",
        "        #print(\"2. Running var:\", net.bn1.running_var)\n",
        "    print('Epoch [{}/50], Loss: {:.4f}'.format(epoch+1, running_loss / len(train_loader)))\n",
        "    #print(\"3. Running mean:\", net.bn1.running_mean)\n",
        "    print(\"===========================\")\n",
        "\n",
        "net.bn1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O0k27Uo7TRE",
        "outputId": "c74fcd4d-e3d8-4bee-fb41-a18fc65608ef"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_______\n",
            "tensor([[[ 9.1874e-02,  1.0531e+00, -1.3493e+00, -1.1954e+00, -5.7552e-01,\n",
            "           5.7898e-01,  7.0092e-02,  1.5063e+00, -5.1719e-02,  5.7271e-01,\n",
            "          -4.2895e-01, -2.4226e-01, -1.5831e+00, -1.0652e+00, -6.2335e-01,\n",
            "           1.7057e-01,  8.0372e-01,  1.1654e+00, -1.0929e+00, -6.6442e-01,\n",
            "           7.4696e-01,  1.5725e+00, -2.5857e-01,  1.4275e+00, -1.7973e-01,\n",
            "           2.9209e-01,  1.7052e+00, -1.5342e+00, -1.0074e+00, -3.4228e-01,\n",
            "          -5.8373e-01,  1.6319e+00, -1.0403e+00, -7.0837e-01, -1.1295e+00,\n",
            "          -1.5499e+00, -9.2645e-01,  1.6241e+00,  8.9364e-01,  9.6159e-01,\n",
            "           1.9804e-01, -8.0088e-01,  4.0408e-01, -1.5449e+00, -1.1718e+00,\n",
            "          -8.0591e-01,  1.2201e+00,  1.1412e+00, -6.7862e-01,  4.1342e-02,\n",
            "           1.2353e+00,  1.8619e+00,  8.0645e-01,  3.4383e-01,  1.2900e+00,\n",
            "          -9.3540e-01,  4.3440e-01, -1.2650e+00, -1.1197e+00, -8.0778e-01,\n",
            "           9.0469e-01,  8.1578e-01, -9.4167e-01,  6.3897e-01],\n",
            "         [ 9.5932e-01, -1.7751e-01,  9.9289e-02,  4.2513e-01,  1.0796e+00,\n",
            "           1.6517e+00, -1.2625e+00, -5.8204e-01,  6.9673e-01,  1.4301e+00,\n",
            "           1.5002e+00,  1.5207e+00,  3.7009e-01, -1.4291e+00,  1.8989e-01,\n",
            "          -1.0184e+00, -1.5342e+00,  1.5310e+00,  1.3152e+00, -1.6446e+00,\n",
            "           3.5015e-01, -2.4864e-01, -2.4208e-01, -7.3574e-01,  6.8249e-01,\n",
            "          -9.6228e-01,  6.5224e-01,  8.8648e-01,  1.2403e+00,  6.6457e-01,\n",
            "          -1.6314e+00, -1.0572e+00,  8.7571e-01,  3.8741e-01, -1.2785e+00,\n",
            "          -9.3452e-01,  1.6190e+00,  1.1695e+00, -6.9915e-01, -3.8877e-01,\n",
            "          -1.5689e+00,  4.7381e-03, -1.2329e+00, -1.2638e+00, -5.7771e-02,\n",
            "           2.8781e-01, -6.5454e-01,  1.0341e+00, -9.8962e-01,  1.5628e+00,\n",
            "           1.1889e+00, -1.3849e+00, -3.8405e-01,  1.1098e-01,  2.8066e-01,\n",
            "           4.3434e-01,  6.9575e-01,  1.3586e-01, -7.8654e-01,  8.3173e-01,\n",
            "          -1.5801e+00, -9.6296e-01, -3.8649e-01, -7.8517e-01],\n",
            "         [-5.3166e-01, -1.3388e+00, -2.9607e-01,  4.3669e-01, -1.0499e+00,\n",
            "          -1.8758e-02,  1.2994e+00, -1.0721e-01,  1.1717e-01, -7.8808e-02,\n",
            "           4.1715e-01,  1.1619e+00,  1.6970e+00,  1.1606e+00,  1.7007e+00,\n",
            "          -5.4844e-02, -1.4741e+00, -7.4514e-01,  1.2393e+00,  5.8280e-02,\n",
            "          -7.8460e-01, -1.2473e+00, -1.5386e+00, -1.3807e+00, -2.7909e-01,\n",
            "           1.0117e+00,  9.9834e-01, -1.5877e+00,  1.1412e+00, -1.2751e+00,\n",
            "          -2.9382e-01, -6.2726e-01, -2.6153e-01, -2.6794e-01, -1.4724e+00,\n",
            "          -1.4141e+00, -1.9944e-01,  9.1638e-02, -7.1111e-01,  7.1666e-01,\n",
            "          -1.4771e+00, -4.6538e-02,  1.5804e+00, -6.3142e-01,  1.6210e+00,\n",
            "           6.9167e-01, -1.4812e+00,  1.1565e+00, -1.2885e-01, -6.9759e-01,\n",
            "           1.4434e+00, -1.3190e+00,  2.5379e-01, -2.9032e-01,  1.2964e+00,\n",
            "           5.4904e-01,  8.9502e-01,  6.7621e-01, -3.4376e-01, -2.9193e-01,\n",
            "          -1.3465e+00,  1.0004e+00,  1.4336e+00,  1.2450e+00],\n",
            "         [-1.2072e+00,  1.9460e-01, -1.2063e+00, -9.1765e-01, -9.7788e-01,\n",
            "           7.4998e-01, -1.0026e+00,  7.0461e-02,  1.8987e-01,  1.3161e+00,\n",
            "          -1.3016e+00, -1.1719e+00, -9.7406e-01,  1.4195e+00, -5.6062e-01,\n",
            "           1.6878e+00,  7.8711e-01,  3.4791e-01,  9.7326e-02, -2.5824e-01,\n",
            "           3.4574e-01, -3.1555e-01,  9.8106e-02,  3.4972e-01, -1.3508e+00,\n",
            "          -8.6840e-01,  1.6205e+00, -1.4056e+00, -2.2971e-02,  1.9602e+00,\n",
            "           7.8641e-01,  1.6416e-01, -1.5085e+00,  1.0371e+00, -1.2201e+00,\n",
            "          -4.1932e-01, -5.1586e-01, -1.6555e-01,  1.3168e-01,  1.6529e+00,\n",
            "           3.4457e-01,  1.7853e+00,  1.2546e+00, -1.0704e+00,  9.4952e-01,\n",
            "          -1.2100e+00, -6.8092e-01,  6.6097e-01,  7.2839e-01,  1.5135e+00,\n",
            "          -4.9044e-01,  1.1423e-01,  1.0735e+00, -1.6963e+00,  1.4625e+00,\n",
            "          -1.4343e+00,  1.3700e-01, -2.0653e-01, -8.7315e-01,  3.5828e-01,\n",
            "           1.6568e+00, -4.3513e-01, -9.9841e-01, -5.8001e-01],\n",
            "         [-1.8443e+00,  9.7270e-01, -8.4663e-01, -1.2120e+00, -1.0338e+00,\n",
            "           1.2140e+00,  1.1254e+00,  1.5900e+00,  7.9955e-01, -5.6093e-01,\n",
            "          -4.5448e-01,  6.6814e-01,  1.6965e+00,  6.2217e-01, -8.3273e-01,\n",
            "          -8.2674e-01, -1.7551e+00,  5.1312e-01, -1.0047e+00, -1.6498e+00,\n",
            "           1.8036e+00, -1.1769e+00, -1.3093e-01,  6.5068e-01,  1.5343e-01,\n",
            "          -1.2228e+00, -1.4873e+00,  1.6478e+00,  4.0921e-01,  1.7113e+00,\n",
            "          -5.6343e-01,  6.6643e-01, -3.5534e-01,  4.3788e-03, -1.0981e+00,\n",
            "           7.5166e-01,  7.0863e-01,  5.0938e-02, -3.4797e-01, -1.1125e+00,\n",
            "           1.4417e+00, -1.3625e+00,  8.9121e-01, -5.6541e-01, -8.5094e-01,\n",
            "           4.4203e-01, -9.2317e-01,  5.4116e-01,  4.7471e-01, -1.3587e+00,\n",
            "           4.1636e-01,  9.2297e-01,  8.6420e-01, -1.5459e-01, -1.5097e+00,\n",
            "          -2.0981e-01,  7.6944e-01, -6.2131e-01,  8.3247e-01,  1.3917e+00,\n",
            "          -9.2843e-01,  1.1048e-01,  8.9870e-01,  2.4428e-01],\n",
            "         [-8.3498e-02, -1.9115e-02, -1.4381e+00, -9.1198e-02,  8.6500e-01,\n",
            "           1.1900e+00,  7.0189e-01, -4.8819e-03,  5.1383e-01, -9.7137e-01,\n",
            "          -1.5044e+00, -1.5530e+00,  1.3403e+00,  7.7516e-01, -1.2903e+00,\n",
            "           3.9580e-01,  9.6266e-01,  1.3434e+00,  1.1484e+00,  9.9317e-01,\n",
            "          -5.7071e-01, -7.6025e-01,  1.0653e+00,  6.5926e-01, -1.1510e+00,\n",
            "           1.1867e+00,  5.3897e-01,  6.3551e-01, -1.1899e+00,  2.3327e-01,\n",
            "          -7.5966e-01,  1.0035e+00, -4.9389e-01,  3.9280e-01,  6.3060e-02,\n",
            "           4.4911e-01, -7.4294e-01,  5.4585e-01, -1.3016e+00,  1.3819e-01,\n",
            "          -1.2675e+00, -1.7503e+00, -1.4645e+00,  1.0161e+00,  6.2912e-01,\n",
            "           1.2478e+00,  1.0342e+00, -1.6046e+00, -1.2766e+00, -4.6530e-01,\n",
            "          -1.2252e+00,  8.6800e-01, -1.3912e+00, -9.7358e-01, -1.7184e+00,\n",
            "          -1.0979e+00,  1.0675e+00,  1.0617e+00,  9.0140e-01,  9.8908e-01,\n",
            "           1.1714e+00, -6.1246e-01,  4.7175e-01,  1.1741e+00],\n",
            "         [ 5.5919e-01,  1.6855e+00,  8.7552e-01,  1.0489e+00, -5.8731e-01,\n",
            "           8.1012e-01,  1.9559e-01, -4.0009e-01, -9.4721e-01, -9.4284e-01,\n",
            "          -1.2937e+00,  1.1326e+00,  1.1993e+00, -1.8958e-01, -9.7243e-01,\n",
            "           1.3037e+00,  1.0790e+00,  1.2726e-01, -7.9300e-01,  1.5498e+00,\n",
            "           6.9074e-01, -1.2763e+00,  1.6138e+00,  1.2809e+00, -6.1172e-01,\n",
            "           9.4876e-01, -9.5475e-01, -2.6182e-01,  1.4320e+00,  7.1666e-02,\n",
            "          -1.1889e+00, -5.6077e-01, -4.5503e-01, -3.2281e-01,  1.6324e-01,\n",
            "           1.5595e+00,  9.2247e-02, -1.0377e+00,  8.8397e-02,  8.0954e-01,\n",
            "           8.3742e-01, -1.5370e+00, -2.9926e-01, -1.2494e+00, -7.1646e-01,\n",
            "           6.0882e-01, -1.1938e+00,  6.2808e-01,  1.4314e+00,  1.1254e-01,\n",
            "          -1.1203e+00, -6.0127e-01,  3.6981e-01, -1.2817e+00,  8.3872e-01,\n",
            "          -1.5268e+00, -1.6167e+00, -1.6342e+00, -3.3954e-01,  1.1345e+00,\n",
            "          -1.5918e+00,  1.4018e+00, -6.7156e-01,  4.9530e-01],\n",
            "         [ 3.4412e-04, -1.5620e+00,  1.2929e+00,  8.1317e-01,  1.5645e+00,\n",
            "           7.5836e-01, -1.1635e+00,  1.3016e+00,  1.3772e-02,  7.0724e-01,\n",
            "          -1.3973e+00, -5.4924e-01, -1.6942e+00, -7.1724e-01,  2.8093e-01,\n",
            "          -1.3698e+00,  4.5086e-01,  8.1047e-01,  1.5489e-01, -1.1784e+00,\n",
            "          -1.3536e+00, -5.8369e-01,  6.4909e-01,  1.5520e+00,  8.7351e-01,\n",
            "          -2.6059e-01,  5.0339e-01, -1.6929e+00, -1.4835e+00,  6.9469e-01,\n",
            "          -1.0090e+00,  7.1859e-01, -1.2391e+00, -8.9177e-01, -1.4333e+00,\n",
            "           7.8579e-01, -2.4292e-01,  1.1944e+00,  9.4582e-01,  8.3774e-01,\n",
            "          -2.1833e-02, -5.8478e-01, -4.3347e-01,  1.4432e-01,  7.4269e-01,\n",
            "          -1.2303e+00,  1.3093e+00, -2.5419e-01, -1.4569e+00, -9.6722e-01,\n",
            "           1.3844e+00,  1.4334e+00, -1.6025e+00,  1.0884e+00,  7.5168e-01,\n",
            "           9.0136e-01,  1.3213e+00, -9.6374e-01,  4.4593e-01,  6.0007e-01,\n",
            "          -5.6221e-01,  4.7706e-01, -5.4938e-01,  9.4481e-01],\n",
            "         [-4.0835e-01, -5.7629e-01,  5.2385e-01,  7.0519e-01, -7.8024e-01,\n",
            "           2.3611e-01,  1.1013e+00, -9.4872e-02,  6.1648e-01, -3.4446e-01,\n",
            "           6.9781e-01, -1.1956e+00, -9.5543e-01, -8.1374e-01,  1.6992e+00,\n",
            "          -3.9360e-01,  7.5584e-02, -7.7306e-01,  1.8389e+00,  1.8008e+00,\n",
            "          -1.2481e+00, -1.3476e+00, -1.2548e+00,  7.1898e-02, -3.6863e-01,\n",
            "          -1.0726e+00, -5.5142e-01,  4.1320e-01, -8.3999e-01,  4.7072e-01,\n",
            "          -3.2355e-01, -7.7318e-01,  3.2240e-01,  1.5288e+00, -1.1105e+00,\n",
            "           1.6219e-01, -5.6920e-01,  1.8090e+00, -1.5909e+00, -1.6083e+00,\n",
            "           1.8162e+00, -1.0341e+00,  4.1812e-01, -7.1637e-02, -2.9068e-01,\n",
            "          -3.1304e-01,  1.1997e+00,  1.6459e-01, -1.6346e+00, -9.5954e-01,\n",
            "          -5.2011e-01,  9.6682e-01, -1.0595e+00,  1.7414e+00, -3.1195e-01,\n",
            "          -2.3591e-01,  1.4606e+00,  9.6575e-01,  1.5744e+00,  1.0993e+00,\n",
            "          -4.5397e-01, -1.0883e+00, -5.0669e-02,  1.5383e+00],\n",
            "         [-5.3477e-01, -1.4773e+00,  6.5276e-01,  1.5504e+00, -4.5364e-01,\n",
            "          -1.3191e+00, -1.4450e+00,  3.9898e-01, -1.2080e-01, -8.0455e-01,\n",
            "           3.2177e-01,  6.0470e-01,  1.2666e+00,  1.2365e+00, -2.3861e-01,\n",
            "          -1.4047e+00, -1.4395e+00,  1.4870e+00,  7.0393e-01,  6.9414e-01,\n",
            "          -1.3783e+00,  1.4748e+00,  1.5088e+00,  1.4379e+00, -1.3439e+00,\n",
            "          -6.0120e-01, -1.0957e+00,  1.5082e+00, -6.8460e-01, -7.2740e-01,\n",
            "           8.3432e-01, -7.3702e-01, -7.8469e-01, -1.3951e-01, -1.5329e-01,\n",
            "          -1.2410e+00,  1.3277e+00, -7.0282e-01,  5.6481e-01,  1.3852e+00,\n",
            "           8.0310e-01,  2.2533e-01,  1.3551e+00,  2.1393e-01, -7.3712e-01,\n",
            "           1.5066e+00,  3.7681e-01, -1.5543e+00, -4.5365e-01, -1.0852e+00,\n",
            "           1.3963e+00, -2.6324e-01, -1.4521e+00, -1.0032e-01,  1.0001e+00,\n",
            "           4.1363e-01,  5.0347e-01,  1.4774e-01,  5.9365e-01, -3.8828e-01,\n",
            "          -6.3382e-01, -1.4890e+00, -1.2001e+00,  6.9015e-01]]])\n",
            "_______\n",
            "tensor([[[-1.1015,  0.1798,  0.4859,  0.0532, -1.1995,  1.5958, -1.0830,\n",
            "           1.3626, -1.4558, -0.4891,  0.8775, -0.7818,  1.3932, -1.2688,\n",
            "          -1.5329, -0.8556,  0.2115,  1.5338, -0.9017,  0.5529, -0.5068,\n",
            "          -0.4479,  1.2107, -1.4201,  0.1149,  1.5680, -0.3095,  1.2824,\n",
            "          -0.6065,  1.3321, -1.7551, -0.3212, -0.3761,  0.4944,  0.6133,\n",
            "           1.2674, -0.3296, -0.1392,  1.1228, -0.5414,  1.4569, -1.2299,\n",
            "          -0.2586, -0.1637,  0.7072,  1.1208, -0.3462, -1.5133,  0.7930,\n",
            "           0.4510,  0.6296, -1.1446,  1.0510,  1.5798,  0.0810, -1.6971,\n",
            "           1.1527, -1.3617,  0.3303,  0.1830, -0.8854,  0.5364, -1.6281,\n",
            "           0.3266],\n",
            "         [-1.0914, -0.2153,  0.1792,  1.3843, -0.8301,  1.0322, -0.2468,\n",
            "          -0.8264, -1.4860, -1.5638,  1.5920, -0.4507,  0.0481,  0.4166,\n",
            "           1.0974,  1.3868,  1.3161, -1.2695, -0.3021, -1.0211, -0.9871,\n",
            "           0.3601,  1.5287, -1.3699,  0.9732, -0.8990, -0.3597,  1.2874,\n",
            "          -0.4120,  0.1166, -1.1998,  1.3831, -0.2505, -1.2537,  0.4527,\n",
            "          -1.2198, -1.3572,  0.0663,  1.2858, -0.5066,  0.7490,  1.3966,\n",
            "           1.0699, -0.9091, -0.2178, -0.5595, -1.3749, -0.0345,  1.2185,\n",
            "           1.3686,  0.1203,  0.3433, -0.7564,  1.2225, -0.7284, -1.1925,\n",
            "           0.2214,  0.9635, -1.5373,  1.4710,  0.5642, -1.2025, -0.3940,\n",
            "           1.4103],\n",
            "         [-1.3912, -0.5247,  0.6067, -1.5897,  1.0085,  0.9580,  1.1398,\n",
            "          -1.1546,  1.8254, -1.0729,  1.8296, -1.7422, -1.6169,  1.1289,\n",
            "           0.1991,  0.1137, -1.0017, -0.7568, -0.5249, -1.7686,  0.1032,\n",
            "           0.3311,  0.0069,  0.5975,  1.7394,  0.4935, -0.6594, -0.7423,\n",
            "          -1.1587, -1.2561, -0.2854, -0.3142,  0.8200, -0.7713,  1.4627,\n",
            "          -1.2716,  0.0136,  1.6483,  1.8224, -1.0483,  0.3334,  0.2239,\n",
            "          -0.8468, -0.6218,  0.5025, -1.2601,  0.3094,  0.7978, -0.7104,\n",
            "           1.5237, -0.1116,  0.8361,  1.8150, -0.5492,  0.9997, -0.5133,\n",
            "          -0.2580, -0.3007, -0.0029, -0.6807,  0.4360,  0.0772,  1.1648,\n",
            "          -0.3608],\n",
            "         [ 0.0620, -0.8088, -1.6670,  0.6232,  0.1130, -0.8613, -0.7463,\n",
            "           0.3593, -0.4563,  1.0819, -0.0185,  0.5735,  0.6804, -0.5753,\n",
            "           0.6748,  1.2377, -0.0381,  0.2379, -0.0559,  1.1536, -0.9014,\n",
            "          -1.1030,  0.6805, -1.7276, -0.0934, -0.9709,  1.0940, -1.8396,\n",
            "           0.9950,  1.7012,  0.2501, -0.2458,  1.6920, -0.9109, -0.2707,\n",
            "          -0.9303, -0.3774,  0.2778,  0.1233, -0.4712, -1.9664, -0.0285,\n",
            "          -0.1775,  0.1544,  1.2899,  1.6692, -1.2168, -0.2379, -0.8617,\n",
            "          -1.8222, -1.4646,  1.6076,  0.9521,  0.8675,  0.9017, -1.5878,\n",
            "           0.0707,  1.5931, -1.1519,  1.5173,  0.3617,  1.1743, -0.9128,\n",
            "           0.7271],\n",
            "         [-1.3579, -0.0761,  0.5422,  0.4898,  1.3260,  1.5532,  1.4396,\n",
            "           1.0684,  1.1177, -0.1782, -1.1859, -0.5554, -0.1521, -0.8189,\n",
            "           0.8189,  0.5695,  0.2741,  0.3990,  1.3909,  0.5900, -1.3430,\n",
            "          -1.1078,  0.7092,  0.2350,  0.7823, -1.5327, -1.4145,  0.9778,\n",
            "           1.0190, -1.3863,  0.6139, -1.1934,  1.2022,  0.5115,  1.5627,\n",
            "          -1.5129, -1.4765,  1.0757,  1.4055,  0.9770,  1.0582, -1.2890,\n",
            "           1.1501, -1.4477,  0.3707, -0.9974, -0.5958,  0.2985, -0.5896,\n",
            "          -0.2237, -1.4534, -1.4457,  0.6261, -0.6365, -0.9248, -0.5918,\n",
            "          -1.2505, -1.2223,  0.9547,  0.8143,  0.2302, -0.8280,  0.5409,\n",
            "           0.0927],\n",
            "         [-1.2493,  0.4334,  1.6916, -1.7359, -1.7554,  0.2758,  1.4066,\n",
            "          -1.0571,  0.4423,  1.3160,  0.6019,  0.4356,  1.8494,  1.2228,\n",
            "          -0.9738,  0.2004,  0.9684,  1.6398, -1.7564,  1.0238, -0.4814,\n",
            "          -1.7280,  0.4456, -0.9412,  0.3004, -1.1309, -1.3178, -0.1096,\n",
            "          -0.5639,  1.1023,  0.2000,  1.4564,  0.3831, -0.5379,  1.2696,\n",
            "           0.1428,  0.3836, -0.5380,  0.0505, -0.3040, -0.1099, -1.3673,\n",
            "           0.0328, -1.4580, -0.3453,  1.0927, -1.2143, -1.1758, -0.3979,\n",
            "          -0.4521,  0.3503,  0.3444,  0.9616, -1.2026, -0.0837,  1.0560,\n",
            "           1.6092, -0.0880, -1.0031,  0.5641,  1.0745, -1.0866, -1.0061,\n",
            "           0.8438],\n",
            "         [-0.1910, -1.6770, -1.0518,  0.6172, -0.6483,  1.1010,  1.2715,\n",
            "           0.1753, -0.2063, -0.6114, -0.1579,  0.2374, -1.3700, -0.4781,\n",
            "          -1.4855,  0.9711, -1.1542,  1.4805, -0.5100, -1.0455,  0.7445,\n",
            "           0.1814,  1.1048,  0.6375, -1.7090,  1.0029, -0.3919, -0.8230,\n",
            "           1.0073,  1.1675, -1.5656, -0.2256,  1.3236,  0.5862,  1.3598,\n",
            "          -0.6543, -1.0094,  1.3894,  0.6364,  1.5595, -0.7818, -1.1630,\n",
            "          -0.2711, -1.6720,  0.5942,  1.1815,  0.3360, -0.2398,  0.7706,\n",
            "          -1.1506,  1.5905, -1.7617, -1.6998, -0.4735,  1.3162,  0.0982,\n",
            "           0.5807, -1.3176,  0.5478, -0.2618,  0.6092,  1.1282,  0.6668,\n",
            "          -0.2165],\n",
            "         [ 0.2940, -0.7267, -0.2476,  1.5779, -0.3186,  1.5259, -0.3105,\n",
            "          -0.0152,  0.4435, -1.2510,  0.3871,  0.2232,  0.2430,  0.0444,\n",
            "           1.8279,  1.7370, -0.8180,  1.2435, -1.0105, -1.1313,  0.9568,\n",
            "           0.4317, -0.4368, -1.3355, -0.2935,  1.0079, -1.2653, -1.3481,\n",
            "           0.6341, -0.8950, -1.1759, -0.2902,  0.1146, -1.2348, -0.7999,\n",
            "           0.5235, -1.2967,  0.1515, -0.7602, -0.8460,  1.1144, -0.9839,\n",
            "          -0.9625,  1.4343,  0.7425,  2.0487, -0.0773, -0.4666,  0.2381,\n",
            "           0.4143,  1.7815, -1.3925,  0.2766,  1.2034, -0.5887, -0.5119,\n",
            "           1.2773, -1.3241,  0.8328, -0.9921,  1.3022, -1.1380,  1.5147,\n",
            "          -1.3033],\n",
            "         [-1.1842,  0.2176,  1.4850, -1.2101, -0.5376,  1.6330, -0.0814,\n",
            "           0.8725, -1.3832, -0.3219, -0.6388,  1.5752,  0.8737, -1.0024,\n",
            "           0.4577,  0.1746, -0.4799, -0.7425, -0.3504,  1.4854, -0.9814,\n",
            "          -0.9740, -0.3122,  0.2220,  0.6396,  0.2427, -0.2011, -0.8003,\n",
            "          -0.1006, -0.4663,  1.5177, -1.4019, -0.8520, -1.1400, -0.1061,\n",
            "           1.4707,  1.6301, -1.2333,  0.1225,  1.1555,  0.9159,  1.5360,\n",
            "           0.1480, -0.9077,  1.2777, -0.3685,  1.0421, -0.2104,  0.8902,\n",
            "           1.6676,  1.7175, -1.2981, -0.8907, -0.4842, -0.8517,  1.7661,\n",
            "           0.0487, -0.8937, -0.5358, -0.7118,  0.8622, -1.2361, -1.3347,\n",
            "          -1.4222],\n",
            "         [ 0.5040,  0.6063, -1.1784,  0.2119,  0.9962,  0.0477, -0.5233,\n",
            "          -0.0666, -0.3563,  0.6207, -1.2711,  0.6265,  0.7169, -1.1541,\n",
            "           1.1216,  1.3865, -0.8155, -0.6329,  0.6125, -1.4381,  0.4479,\n",
            "           0.1885,  0.8236, -1.5744, -1.5139,  0.5338,  0.3915, -1.6738,\n",
            "          -1.0769, -1.6486, -1.3309, -0.9066,  1.1391, -0.5032,  0.7341,\n",
            "           1.3345,  0.5443, -1.6169,  0.6203,  0.4255,  1.0552, -1.7629,\n",
            "           0.7991,  1.1247, -1.6732, -0.0174,  1.1809, -1.3483,  0.3402,\n",
            "           0.5222,  0.2345,  1.4059,  1.1251,  1.1878, -0.3842,  1.1570,\n",
            "          -1.6757, -1.0481, -0.5269, -0.6600,  1.2930,  0.5698,  0.8452,\n",
            "           0.9034]]])\n",
            "_______\n",
            "tensor([[[-1.2727, -0.1257,  0.3501,  0.5574,  1.0193, -0.6383, -0.4632,\n",
            "           1.6053, -0.4029, -1.1776,  1.3169,  1.3153, -0.4851, -0.2785,\n",
            "          -1.2561, -1.4328,  0.7681, -0.6915,  1.5315, -1.3703, -0.1664,\n",
            "           0.1240,  1.0187,  1.5555,  1.3154,  0.8847, -0.7493, -1.2130,\n",
            "          -1.5410,  0.1749,  1.8473, -0.2604,  0.0083, -1.2358,  0.6427,\n",
            "           1.2369, -1.7069, -1.1948,  1.4308,  0.5150, -0.0285,  0.4096,\n",
            "          -0.1915, -1.1854, -1.6133,  1.2800, -0.1314, -0.5901, -0.3843,\n",
            "          -0.9064, -0.4535, -1.4752,  1.4447,  0.8268, -0.8317, -0.5355,\n",
            "           0.9001, -0.5882,  0.5640,  0.5851, -1.0592,  0.5400,  0.1960,\n",
            "           1.6719],\n",
            "         [-0.8801, -0.6967,  1.2617, -1.6762, -0.1202,  1.4401, -1.0316,\n",
            "           1.4316,  1.3067,  1.3820, -0.4137,  1.3370, -1.0094, -0.8459,\n",
            "           0.4155,  0.1232,  0.7184,  1.4170, -1.3425,  0.6824,  0.3390,\n",
            "           0.6769,  0.5834, -0.9015, -1.6303,  1.1545, -1.2449,  0.8726,\n",
            "          -1.9144, -0.5495, -0.2281,  0.0562, -0.1051,  0.3050, -0.4416,\n",
            "           0.1821,  1.4857, -1.3794, -0.3422, -0.9149,  1.2894,  1.2519,\n",
            "          -0.2944, -0.8535, -1.3283,  1.0060, -0.3672,  1.4021,  0.7517,\n",
            "           1.2857,  1.1379, -1.3843,  0.2658, -0.9726,  0.6317, -0.4040,\n",
            "          -0.0951, -1.4574, -1.8112,  0.4126,  0.1602, -0.0946,  0.9424,\n",
            "          -0.9777],\n",
            "         [ 0.8050, -0.7740,  0.5473, -0.2119,  1.4070,  1.7106,  1.7002,\n",
            "           0.2645,  1.2141, -1.5557, -0.8365,  0.4977,  0.1958, -1.1825,\n",
            "          -0.5855, -0.6281, -1.5411,  0.3524,  1.3745,  0.8937, -0.6422,\n",
            "          -1.4147, -1.2203,  0.2185,  1.3854, -0.5199, -0.7625, -0.7438,\n",
            "           0.0352, -1.4694, -1.3522,  1.9022, -0.6453, -0.1283, -0.2803,\n",
            "          -1.3454, -0.1212,  0.7198, -0.5218,  0.1118,  0.1778, -1.5493,\n",
            "           1.4530,  1.5943,  1.3747, -0.3864,  1.0971, -1.4472, -0.2451,\n",
            "          -0.1684, -0.4295,  0.9183, -0.6082, -0.0159, -0.3173, -1.4510,\n",
            "          -0.1018, -1.2913,  1.2026,  1.1178,  1.8192, -0.3993,  0.2748,\n",
            "           0.5276],\n",
            "         [ 1.5810,  1.4460, -0.9262, -1.4777, -1.5610, -0.8667,  1.3206,\n",
            "           0.4854,  1.5258,  1.1559, -0.0462, -1.0953,  1.5451,  1.2567,\n",
            "           0.6460, -0.9737,  1.2329,  0.0804,  1.0236,  0.6202,  0.8082,\n",
            "          -1.5216, -0.9873,  0.9219, -0.4148, -1.5092,  1.4341, -0.1059,\n",
            "           1.2642, -1.3631,  0.4994, -0.6932, -0.7662, -0.4152, -0.6824,\n",
            "           0.8362, -0.3844,  1.1840,  0.3073, -0.3257, -1.0256, -1.5848,\n",
            "          -0.5527, -1.6019, -0.3766, -0.0431, -0.1768, -0.8827,  1.0779,\n",
            "           0.8614, -0.4777, -0.7237,  0.9698,  1.3690, -1.5847, -0.4275,\n",
            "          -0.2237, -0.6691,  1.3420, -0.3145,  1.3323, -0.0620, -0.2496,\n",
            "          -1.0350],\n",
            "         [-0.5214,  0.6842,  0.9054,  0.0254,  1.8454, -1.1836,  0.7969,\n",
            "           1.7493, -0.2293,  1.5527,  0.1212, -1.2880, -0.9193,  0.7762,\n",
            "          -0.7060,  0.5337,  1.9032,  0.7163, -0.0815,  0.5511, -0.9700,\n",
            "          -1.4563, -1.2933, -1.4542,  0.4687,  0.4495, -0.6916, -1.7102,\n",
            "           1.0883, -0.9539, -0.2165, -1.4530, -0.9479,  0.8590, -1.6247,\n",
            "          -0.9591,  1.4714, -0.8535, -0.6754, -0.4261,  0.8780, -1.1170,\n",
            "           0.2755,  0.8429, -0.0428,  0.4238, -1.3138,  0.9233,  0.7790,\n",
            "          -0.1401, -1.4317,  0.8274,  1.2924, -0.2312,  0.9588,  0.7734,\n",
            "          -0.6456, -0.0370,  1.9238,  0.0996, -1.3750, -0.0434,  1.0069,\n",
            "          -0.5102],\n",
            "         [-0.9909,  1.4496,  0.9157, -0.3501,  0.0501, -0.2340,  0.3781,\n",
            "           0.0220, -1.1384,  0.2932,  0.0050,  0.0747, -0.7326,  0.4217,\n",
            "          -1.5070,  0.4990,  0.9390, -0.2417, -1.4493, -0.0490, -0.0398,\n",
            "          -1.6076, -1.3234,  1.6328,  1.3120,  0.9613,  1.2123,  0.6548,\n",
            "           0.9769,  1.1571, -0.4655, -0.2726,  1.2490, -0.0431, -1.2209,\n",
            "          -1.4467,  0.9705,  1.4578,  1.3641,  0.2590,  1.2078, -1.7053,\n",
            "          -1.3473, -1.7851,  1.4168, -0.0588, -0.8764,  0.5704, -0.7204,\n",
            "          -1.2040,  0.6561,  0.2892, -1.3394, -1.2353,  0.8068,  1.3347,\n",
            "           0.1050, -0.9521,  0.8321,  0.2735, -1.6280, -1.2161,  0.3540,\n",
            "           1.0783],\n",
            "         [-0.5915,  0.7812, -1.0998,  1.5887,  1.1718,  0.2760,  0.7153,\n",
            "           0.5503, -1.0500, -0.0584,  0.4113, -0.4285,  1.5921, -0.7013,\n",
            "          -1.5668, -0.8555, -1.6265,  1.3620, -0.8092, -1.2436, -0.2279,\n",
            "          -1.1928, -0.9345,  0.8260, -1.3241,  0.7608,  1.2003, -0.0474,\n",
            "           0.7138, -0.9990,  1.2866,  1.6134,  0.3618,  1.5108, -1.7375,\n",
            "          -0.2308,  0.2954, -1.5260,  0.7865, -0.6936, -0.6604, -1.3968,\n",
            "           0.6538, -1.2846, -0.4055,  0.7142,  1.2414,  0.3422, -0.6798,\n",
            "          -0.7854,  0.9138,  0.2729,  1.3923,  0.5988,  1.3305, -1.4179,\n",
            "           1.3277, -0.8272, -0.5001, -0.0333, -0.3634,  0.2939,  1.4764,\n",
            "          -1.0626],\n",
            "         [ 0.5306, -1.4979,  0.2536,  1.4692,  1.1966,  1.5285,  0.9542,\n",
            "          -0.1835,  0.4433, -0.7944,  1.5148,  0.8801, -1.3670,  1.5436,\n",
            "          -0.0597, -0.1137, -0.5316, -0.5137,  0.0586, -0.9227, -1.7285,\n",
            "          -0.9717, -0.3955,  1.3440,  1.3085, -1.1250, -0.1027, -0.5588,\n",
            "          -1.0079, -0.3533, -0.6652,  0.9863, -0.9368,  0.2478, -1.0614,\n",
            "          -0.6017,  1.4025, -0.9277, -0.1423,  1.7084,  0.5913, -0.9606,\n",
            "          -1.1272, -0.6720, -0.6036,  0.7611, -0.4380,  1.5049, -1.0329,\n",
            "          -0.6027,  0.6311,  1.4233,  1.7112,  0.2591,  0.0442, -0.8495,\n",
            "           0.9974, -1.0894, -0.5119, -1.5564, -1.7852,  0.9863,  0.0912,\n",
            "           1.4206],\n",
            "         [ 1.3371,  0.8524,  0.6296, -0.5504,  0.8638,  0.7739,  0.5330,\n",
            "          -0.1378,  0.1810, -0.0783, -1.3843,  0.3290, -1.4011,  0.7817,\n",
            "          -1.1519, -0.6705, -0.9345,  1.3256, -1.3458,  0.8271,  0.8109,\n",
            "           1.1785,  0.3620,  0.8889,  1.3277,  0.2661,  0.5220, -1.4848,\n",
            "          -0.5660,  0.0760,  0.1305, -0.7181, -1.8594, -0.5514,  0.5986,\n",
            "           1.4540, -1.7014,  0.7678, -1.7834, -1.7967, -1.2064, -0.3225,\n",
            "           0.4134, -1.4465,  1.2919,  1.5313,  0.0238,  1.3697, -0.8960,\n",
            "          -0.7938, -0.3170,  0.1092, -1.3340, -1.5185,  0.6204,  0.4295,\n",
            "           1.6636, -0.8155,  0.5454, -0.8080,  0.7968, -0.1853,  1.2398,\n",
            "           0.9075],\n",
            "         [ 1.3539,  0.8091, -0.0059,  1.4042, -1.0750,  1.5435,  1.6017,\n",
            "           1.5642, -0.8693, -0.9766,  0.0381,  0.4626,  0.7881,  0.6257,\n",
            "           1.3866, -0.4988, -0.6314,  0.5033,  0.4066, -1.0681,  0.0954,\n",
            "          -1.3691, -0.5720, -0.7416,  1.4296, -1.6102, -1.0803, -1.7444,\n",
            "           0.3614, -1.3869,  0.4834, -1.2585, -1.1661, -0.9442,  0.1866,\n",
            "           1.5019,  1.5324, -0.1994, -1.7459, -0.6353, -0.7221, -0.4340,\n",
            "           0.5836,  0.7908,  0.6884,  0.4186, -0.7627, -0.0883,  0.1476,\n",
            "          -0.9440, -0.2232, -0.2837, -1.6866,  1.0897, -0.2431, -1.5491,\n",
            "           1.2834, -0.9907,  0.5356,  1.6265,  0.0091,  0.4221,  0.6096,\n",
            "           1.2229]]])\n",
            "Epoch [1/50], Loss: 7.3254\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('running_mean',\n",
              "              tensor([0.4767, 0.5704, 0.4466, 0.5212, 0.4753, 0.5117, 0.5128, 0.5090, 0.5241,\n",
              "                      0.5205])),\n",
              "             ('running_var',\n",
              "              tensor([0.0769, 0.0829, 0.0836, 0.0928, 0.0749, 0.0774, 0.0873, 0.0764, 0.0794,\n",
              "                      0.0868])),\n",
              "             ('num_batches_tracked', tensor(3))])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_bn_training(m):\n",
        "    if isinstance(m, nn.BatchNorm1d):\n",
        "        m.train()\n",
        "\n",
        "torch.manual_seed(1)\n",
        "test_sample = torch.randn(1, 10, 64)\n",
        "#print(test_sample)\n",
        "#print(torch.mean(test_sample, dim=2))\n",
        "net.eval()\n",
        "print(net.bn1.training)\n",
        "print(\"After Running\", net.bn1.state_dict())\n",
        "\n",
        "net.apply(set_bn_training)\n",
        "output = net(test_sample)\n",
        "print(\"Before Running\", net.bn1.state_dict())\n",
        "print(net.bn1.training)\n",
        "\n",
        "print('\\n', '-----------------------')\n",
        "print(net.bn1.state_dict())\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rkk_s6J7Z3S",
        "outputId": "be50c68d-efc7-4f9e-f45a-fdb0a01c7b9a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "After Running OrderedDict([('running_mean', tensor([0.4767, 0.5704, 0.4466, 0.5212, 0.4753, 0.5117, 0.5128, 0.5090, 0.5241,\n",
            "        0.5205])), ('running_var', tensor([0.0769, 0.0829, 0.0836, 0.0928, 0.0749, 0.0774, 0.0873, 0.0764, 0.0794,\n",
            "        0.0868])), ('num_batches_tracked', tensor(45))])\n",
            "Before Running OrderedDict([('running_mean', tensor([-0.1130, -0.0192,  0.1874,  0.1517, -0.0068, -0.0377, -0.0822, -0.1019,\n",
            "         0.1284, -0.0743])), ('running_var', tensor([1.0302, 0.9497, 1.2041, 1.1384, 1.1395, 0.9414, 0.9778, 1.0286, 0.6879,\n",
            "        1.3984])), ('num_batches_tracked', tensor(46))])\n",
            "True\n",
            "\n",
            " -----------------------\n",
            "OrderedDict([('running_mean', tensor([-0.1130, -0.0192,  0.1874,  0.1517, -0.0068, -0.0377, -0.0822, -0.1019,\n",
            "         0.1284, -0.0743])), ('running_var', tensor([1.0302, 0.9497, 1.2041, 1.1384, 1.1395, 0.9414, 0.9778, 1.0286, 0.6879,\n",
            "        1.3984])), ('num_batches_tracked', tensor(46))])\n",
            "tensor([[-1.4253]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Batchnorm\n",
        "torch.manual_seed(1)\n",
        "test_sample = torch.randn(1, 10, 64)\n",
        "\n",
        "mean = torch.mean(test_sample, dim=2).reshape(1,10,1)\n",
        "var = torch.var(test_sample, dim=2, unbiased=False).reshape(1,10,1)\n",
        "\n",
        "norm = (test_sample-mean)/torch.sqrt(var + net.bn1.eps)\n",
        "#norm = (test_sample-net.bn1.running_mean.reshape(1,10,1))/torch.sqrt(net.bn1.running_var.reshape(1,10,1)*63/64 + net.bn1.eps)\n",
        "\n",
        "#test = net.bn1(test_sample)\n",
        "results = net.gamma * norm + net.beta\n",
        "net.fc1(results.reshape(-1,640))\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-mSBcFi7chi",
        "outputId": "32f7eb18-7eca-494b-a130-5aa80f2733ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.4253]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(0)\n",
        "# Define the SimpleNet model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        # self.ln1 = nn.LayerNorm(64, elementwise_affine=False)\n",
        "        self.ln1 = nn.LayerNorm(64, elementwise_affine = True)\n",
        "        self.fc1 = nn.Linear(640, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x) #input(1,10,64)\n",
        "        #x = x[0,0,0]\n",
        "        x = x.view(-1, 640)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Generate a simulated dataset\n",
        "inputs = torch.rand(3, 10, 64)\n",
        "raw_inputs = inputs\n",
        "labels = torch.randint(low=0, high=10, size=(3,))\n",
        "inputs = torch.Tensor(inputs)\n",
        "labels = torch.Tensor(labels).long()\n",
        "train_dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
        "\n",
        "# Create a DataLoader to load and batch the data\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "net = SimpleNet()\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(15):\n",
        "    running_loss = 0.0\n",
        "    #print(\"1. Running mean:\", net.bn1.running_mean)\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        #print(\"2. Running mean:\", net.bn1.running_mean)\n",
        "        #print(\"2. Running var:\", net.bn1.running_var)\n",
        "    print('Epoch [{}/50], Loss: {:.4f}'.format(epoch+1, running_loss / len(train_loader)))\n",
        "    #print(\"3. Running mean:\", net.bn1.running_mean)\n",
        "    print(\"===========================\")\n",
        "\n",
        "print('\\n', net.ln1.state_dict(), '\\n')\n",
        "\n",
        "torch.manual_seed(1)\n",
        "test_sample = torch.randn(1, 10, 64)\n",
        "net.eval()\n",
        "output = net(test_sample)\n",
        "print('test results: ', output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEJGo1PA7e-y",
        "outputId": "a766c0e4-3a89-4b27-8415-d4c258ad2c98"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 7.3254\n",
            "===========================\n",
            "Epoch [2/50], Loss: 5.4234\n",
            "===========================\n",
            "Epoch [3/50], Loss: 2.1422\n",
            "===========================\n",
            "Epoch [4/50], Loss: 0.8567\n",
            "===========================\n",
            "Epoch [5/50], Loss: 1.5165\n",
            "===========================\n",
            "Epoch [6/50], Loss: 0.7181\n",
            "===========================\n",
            "Epoch [7/50], Loss: 1.5302\n",
            "===========================\n",
            "Epoch [8/50], Loss: 1.5132\n",
            "===========================\n",
            "Epoch [9/50], Loss: 0.3236\n",
            "===========================\n",
            "Epoch [10/50], Loss: 1.1005\n",
            "===========================\n",
            "Epoch [11/50], Loss: 1.7108\n",
            "===========================\n",
            "Epoch [12/50], Loss: 0.8583\n",
            "===========================\n",
            "Epoch [13/50], Loss: 1.4139\n",
            "===========================\n",
            "Epoch [14/50], Loss: 1.4305\n",
            "===========================\n",
            "Epoch [15/50], Loss: 0.2207\n",
            "===========================\n",
            "\n",
            " OrderedDict([('weight', tensor([0.9926, 0.9922, 0.9954, 0.9899, 0.9929, 0.9900, 0.9937, 0.9915, 0.9968,\n",
            "        0.9933, 0.9925, 0.9940, 0.9920, 0.9938, 0.9890, 0.9957, 0.9940, 0.9891,\n",
            "        0.9928, 0.9924, 0.9961, 0.9885, 0.9931, 0.9941, 0.9921, 0.9945, 0.9931,\n",
            "        0.9899, 0.9929, 0.9914, 0.9896, 0.9914, 0.9948, 0.9977, 0.9927, 0.9930,\n",
            "        0.9894, 0.9931, 0.9940, 0.9930, 0.9928, 0.9858, 0.9927, 0.9925, 0.9920,\n",
            "        0.9932, 0.9935, 0.9918, 0.9942, 0.9920, 0.9900, 0.9926, 0.9884, 0.9924,\n",
            "        0.9932, 0.9922, 0.9893, 0.9938, 0.9939, 0.9949, 0.9929, 0.9969, 0.9941,\n",
            "        0.9958])), ('bias', tensor([ 3.5350e-03,  4.8383e-03,  1.9642e-03, -2.9608e-03,  1.7931e-03,\n",
            "        -7.5735e-03, -9.3407e-04, -5.4459e-03,  1.4767e-03,  2.5330e-03,\n",
            "         5.3152e-03, -1.4077e-03,  3.7877e-04,  3.5698e-03,  6.0505e-04,\n",
            "        -4.3456e-03,  9.2024e-04, -9.2958e-03, -3.7180e-03,  1.7247e-03,\n",
            "         2.1949e-03,  3.2257e-03, -1.5099e-03,  3.2636e-03,  6.7580e-04,\n",
            "         9.6288e-05, -3.4961e-04,  4.3288e-03, -2.2005e-03,  1.1635e-03,\n",
            "         4.6008e-03, -1.8061e-03, -1.0888e-03,  2.4543e-04, -5.0185e-05,\n",
            "         3.5953e-03,  2.2348e-03, -1.8734e-03, -1.7273e-03, -2.7582e-03,\n",
            "        -5.8091e-03,  9.3735e-03, -8.3777e-03,  3.4869e-04, -2.3448e-03,\n",
            "        -5.9207e-03, -2.6729e-03, -2.6188e-03,  3.1880e-03,  5.4464e-04,\n",
            "        -7.3308e-03,  6.9337e-04, -5.0590e-04, -5.0934e-04,  1.3944e-03,\n",
            "         2.5200e-03, -5.7123e-03,  6.0056e-03,  1.0967e-03, -2.2186e-03,\n",
            "         8.6891e-05,  1.7228e-03, -1.0465e-03, -4.4639e-03]))]) \n",
            "\n",
            "test results:  tensor([[-1.4253]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "# Defince the customer batchnorm\n",
        "class CustomBatchNorm(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=1):\n",
        "        super(CustomBatchNorm, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Initialize learnable parameters\n",
        "        #self.gamma = nn.Parameter(torch.ones(num_features), requires_grad=False)\n",
        "        #self.beta = nn.Parameter(torch.zeros(num_features), requires_grad=False)\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "        self.register_buffer('running_var', torch.ones(num_features))\n",
        "\n",
        "\n",
        "    def forward_(self, x):\n",
        "        # Calculate batch statistics\n",
        "        batch_mean = x.mean(dim=2)\n",
        "        batch_var = x.var(dim=2, unbiased=False)\n",
        "\n",
        "        # Update running statistics\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "        # Normalize input\n",
        "        x = (x - batch_mean.unsqueeze(dim=2)) / torch.sqrt(batch_var.unsqueeze(dim=2) + self.eps)\n",
        "\n",
        "        # Apply scale and shift\n",
        "        #x = self.gamma * x + self.beta\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate batch statistics\n",
        "        if self.training:\n",
        "            #batch_mean = x.mean(dim=2)\n",
        "            #batch_var = x.var(dim=2, unbiased=False)\n",
        "            batch_mean = x.mean([0, 2])\n",
        "            batch_var = x.var([0, 2], unbiased=False)\n",
        "\n",
        "            # Update running statistics\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "        # else:\n",
        "        #     batch_mean = x.mean([0, 2])\n",
        "        #     batch_var = x.var([0, 2], unbiased=False)\n",
        "        #     with torch.no_grad():\n",
        "        #         self.running_mean = batch_mean\n",
        "        #         self.running_var = batch_var\n",
        "           \n",
        "        # Normalize input\n",
        "        x = F.batch_norm(x, running_mean=self.running_mean, running_var=self.running_var, \n",
        "                  training=True, momentum=self.momentum, eps=self.eps)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define the SimpleNet model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        #self.bn1 = BatchNorm1d(10, track_running_stats=True, affine=False, momentum=1)\n",
        "        self.bn1 = CustomBatchNorm(10)\n",
        "        self.fc1 = nn.Linear(640, 1)\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.ones(64))\n",
        "        self.beta = nn.Parameter(torch.zeros(64))\n",
        "\n",
        "    def Layer_norm1(self, x):\n",
        "        batch, sentence_length, embedding_dim = x.shape\n",
        "        x = x.reshape(1, batch*sentence_length, embedding_dim)\n",
        "\n",
        "        output = self.bn1(x) \n",
        "        output = output.reshape(output.shape[1], output.shape[2]).reshape(batch, sentence_length, embedding_dim)\n",
        "\n",
        "        #apply gamma and beta\n",
        "        return self.gamma * output + self.beta\n",
        "\n",
        "\n",
        "    def loop_layernorm1(self, x):      \n",
        "        split_x = torch.split(x,1,dim=0)\n",
        "        concate_list = []\n",
        "        for small_x in split_x:\n",
        "            #print(\"before\",small_x.shape)\n",
        "            #small_x = small_x.squeeze().permute(1,0)\n",
        "            #small_x = small_x.permute(1,0)\n",
        "            #small_x = small_x.permute(0,2,1)\n",
        "            small_x = self.bn1(small_x)\n",
        "            #small_x = small_x.permute(0,2,1)\n",
        "            #print(\"after\",small_x.shape)\n",
        "            # print(\"_______\")\n",
        "            # print(small_x)\n",
        "            concate_list.append(small_x)\n",
        "    \n",
        "        concate = torch.stack(concate_list)\n",
        "        return self.gamma * concate + self.beta\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.loop_layernorm1(x) #input(1,10,64)\n",
        "        print(x)\n",
        "        x = x.reshape(-1, 640)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Generate a simulated dataset\n",
        "inputs = torch.rand(3, 10, 64)\n",
        "raw_inputs = inputs\n",
        "labels = torch.randint(low=0, high=10, size=(3,))\n",
        "inputs = torch.Tensor(inputs)\n",
        "labels = torch.Tensor(labels).long()\n",
        "train_dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
        "\n",
        "# Create a DataLoader to load and batch the data\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=False)\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "net = SimpleNet()\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1):\n",
        "    running_loss = 0.0\n",
        "    #print(\"1. Running mean:\", net.bn2.state_dict())\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        #print(\"2. Running mean:\", net.bn1.running_mean)\n",
        "        #print(\"2. Running var:\", net.bn1.running_var)\n",
        "    print('Epoch [{}/50], Loss: {:.4f}'.format(epoch+1, running_loss / len(train_loader)))\n",
        "    #print(\"3. Running mean:\", net.bn1.running_mean)\n",
        "    print(\"===========================\")\n",
        "\n",
        "net.bn1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya-EeDP0_O2K",
        "outputId": "87227907-7472-4c66-fab0-235a256a7b64"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 9.1874e-02,  1.0531e+00, -1.3493e+00,  ...,  8.1578e-01,\n",
            "           -9.4167e-01,  6.3897e-01],\n",
            "          [ 9.5932e-01, -1.7751e-01,  9.9289e-02,  ..., -9.6296e-01,\n",
            "           -3.8649e-01, -7.8517e-01],\n",
            "          [-5.3166e-01, -1.3388e+00, -2.9607e-01,  ...,  1.0004e+00,\n",
            "            1.4336e+00,  1.2450e+00],\n",
            "          ...,\n",
            "          [ 3.4412e-04, -1.5620e+00,  1.2929e+00,  ...,  4.7706e-01,\n",
            "           -5.4938e-01,  9.4481e-01],\n",
            "          [-4.0835e-01, -5.7629e-01,  5.2385e-01,  ..., -1.0883e+00,\n",
            "           -5.0669e-02,  1.5383e+00],\n",
            "          [-5.3477e-01, -1.4773e+00,  6.5276e-01,  ..., -1.4890e+00,\n",
            "           -1.2001e+00,  6.9015e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.1015e+00,  1.7981e-01,  4.8591e-01,  ...,  5.3643e-01,\n",
            "           -1.6281e+00,  3.2664e-01],\n",
            "          [-1.0914e+00, -2.1531e-01,  1.7916e-01,  ..., -1.2025e+00,\n",
            "           -3.9404e-01,  1.4103e+00],\n",
            "          [-1.3912e+00, -5.2474e-01,  6.0675e-01,  ...,  7.7178e-02,\n",
            "            1.1648e+00, -3.6079e-01],\n",
            "          ...,\n",
            "          [ 2.9396e-01, -7.2675e-01, -2.4764e-01,  ..., -1.1380e+00,\n",
            "            1.5147e+00, -1.3033e+00],\n",
            "          [-1.1842e+00,  2.1763e-01,  1.4850e+00,  ..., -1.2361e+00,\n",
            "           -1.3347e+00, -1.4222e+00],\n",
            "          [ 5.0399e-01,  6.0627e-01, -1.1784e+00,  ...,  5.6981e-01,\n",
            "            8.4521e-01,  9.0344e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.2727e+00, -1.2570e-01,  3.5013e-01,  ...,  5.3999e-01,\n",
            "            1.9596e-01,  1.6719e+00],\n",
            "          [-8.8006e-01, -6.9668e-01,  1.2617e+00,  ..., -9.4590e-02,\n",
            "            9.4235e-01, -9.7766e-01],\n",
            "          [ 8.0496e-01, -7.7401e-01,  5.4733e-01,  ..., -3.9928e-01,\n",
            "            2.7477e-01,  5.2762e-01],\n",
            "          ...,\n",
            "          [ 5.3062e-01, -1.4979e+00,  2.5357e-01,  ...,  9.8632e-01,\n",
            "            9.1176e-02,  1.4206e+00],\n",
            "          [ 1.3371e+00,  8.5241e-01,  6.2961e-01,  ..., -1.8529e-01,\n",
            "            1.2398e+00,  9.0753e-01],\n",
            "          [ 1.3539e+00,  8.0907e-01, -5.8505e-03,  ...,  4.2209e-01,\n",
            "            6.0964e-01,  1.2229e+00]]]], grad_fn=<AddBackward0>)\n",
            "Epoch [1/50], Loss: 7.3685\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('running_mean',\n",
              "              tensor([0.4767, 0.5704, 0.4466, 0.5212, 0.4753, 0.5117, 0.5128, 0.5090, 0.5241,\n",
              "                      0.5205])),\n",
              "             ('running_var',\n",
              "              tensor([0.0769, 0.0829, 0.0836, 0.0928, 0.0749, 0.0774, 0.0873, 0.0764, 0.0794,\n",
              "                      0.0868]))])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0oeK7VGKzzuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = SimpleNet()\n",
        "net.eval()\n",
        "\n",
        "x = torch.randn(1, 10, 64) \n",
        "\n",
        "# Export the model to ONNX format\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(net, x,'testtest.onnx',\\\n",
        "    verbose=False,opset_version=15,\\\n",
        "    input_names=[\"input\"], output_names=['output'])"
      ],
      "metadata": {
        "id": "P1qyMGvJ4JDw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "lTBBJV2YOnCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "test_sample = torch.randn(1, 10, 64)\n",
        "\n",
        "net.eval()\n",
        "output = net(test_sample)\n",
        "print(output)\n",
        "print(net.bn1.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BhL0OisTK0H",
        "outputId": "5f7249dd-b61f-4795-8df0-4b217a8e8ed6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.6148]], grad_fn=<AddmmBackward0>)\n",
            "OrderedDict([('running_mean', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), ('running_var', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = torch.randn(64, 10)\n",
        "test_sample = test_sample.unsqueeze(dim=0)\n",
        "test_sample.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkZhHhTcPLpW",
        "outputId": "dc145a60-f13f-44c7-9ea2-db2eb386cb5e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "歡迎使用 Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}